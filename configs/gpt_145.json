{
	"batch_size": 128,
	"n_grad_accumulation_steps": 2,
	"fp16": "true",
	"eval_freq": 256000,
	"print_freq": 25600,
	"emb_dim": 768,
	"n_layers": 20,
	"dropout": 0.1,
	"transformer_ffn_emb_dim": 3072,
	"attention_heads": 12,
	"relu_dropout": 0.1,
	"attention_dropout": 0.1,
	"max_length": 512,
	"n_epochs": 9999999,
	"n_warmup_steps": 20000,
	"reload": "true",
	"activation": "gelu",
	"adaptive_inputs": "false",
	"adaptive_softmax": "false",
	"cache": "true"
}
